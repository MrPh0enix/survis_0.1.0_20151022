@article{bermudez_synthesis_2019,
	title = {Synthesis of {Multispectral} {Optical} {Images} {From} {SAR}/{Optical} {Multitemporal} {Data} {Using} {Conditional} {Generative} {Adversarial} {Networks}},
	volume = {16},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1545-598X, 1558-0571},
	url = {https://ieeexplore.ieee.org/document/8637007/},
	doi = {10.1109/LGRS.2019.2894734},
	abstract = {The synthesis of realistic data using deep learning techniques has greatly improved the performance of classifiers in handling incomplete data. Remote sensing applications that have profited from those techniques include translating images of different sensors, improving the image resolution and completing missing temporal or spatial data such as in cloudy optical images. In this context, this letter proposes a new deep-learning-based framework to synthesize missing or corrupted multispectral optical images using multimodal/multitemporal data. Specifically, we use conditional generative adversarial networks (cGANs) to generate the missing optical image by exploiting the correspondent synthetic aperture radar (SAR) data with a SAR-optical data from the same area at a different acquisition date. The proposed framework was evaluated in two land-cover applications over tropical regions, where cloud coverage is a major problem: crop recognition and wildfire detection. In both applications, our proposal was superior to alternative approaches tested in our experiments. In particular, our approach outperformed recent cGAN-based proposals for cloud removal, on average, by 7.7\% and 8.6\% in terms of overall accuracy and F1-score, respectively.},
	number = {8},
	urldate = {2024-05-16},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	author = {Bermudez, Jose D. and Happ, Patrick N. and Feitosa, Raul Q. and Oliveira, Dario A. B.},
	month = aug,
	year = {2019},
	pages = {1220--1224},
}

@article{ebel_uncrtaints_2023,
	title = {{UnCRtainTS}: {Uncertainty} {Quantification} for {Cloud} {Removal} in {Optical} {Satellite} {Time} {Series}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {{UnCRtainTS}},
	url = {https://arxiv.org/abs/2304.05464},
	doi = {10.48550/ARXIV.2304.05464},
	abstract = {Clouds and haze often occlude optical satellite images, hindering continuous, dense monitoring of the Earth's surface. Although modern deep learning methods can implicitly learn to ignore such occlusions, explicit cloud removal as pre-processing enables manual interpretation and allows training models when only few annotations are available. Cloud removal is challenging due to the wide range of occlusion scenarios -- from scenes partially visible through haze, to completely opaque cloud coverage. Furthermore, integrating reconstructed images in downstream applications would greatly benefit from trustworthy quality assessment. In this paper, we introduce UnCRtainTS, a method for multi-temporal cloud removal combining a novel attention-based architecture, and a formulation for multivariate uncertainty prediction. These two components combined set a new state-of-the-art performance in terms of image reconstruction on two public cloud removal datasets. Additionally, we show how the well-calibrated predicted uncertainties enable a precise control of the reconstruction quality.},
	urldate = {2024-05-16},
	author = {Ebel, Patrick and Garnot, Vivien Sainte Fare and Schmitt, Michael and Wegner, Jan Dirk and Zhu, Xiao Xiang},
	year = {2023},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Image and Video Processing (eess.IV)},
}

@article{meraner_cloud_2020,
	title = {Cloud removal in {Sentinel}-2 imagery using a deep residual neural network and {SAR}-optical data fusion},
	volume = {166},
	issn = {09242716},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0924271620301398},
	doi = {10.1016/j.isprsjprs.2020.05.013},
	abstract = {Optical remote sensing imagery is at the core of many Earth observation activities. The regular, consistent and global-scale nature of the satellite data is exploited in many applications, such as cropland monitoring, climate change assessment, land-cover and land-use classification, and disaster assessment. However, one main problem severely affects the temporal and spatial availability of surface observations, namely cloud cover. The task of removing clouds from optical images has been subject of studies since decades. The advent of the Big Data era in satellite remote sensing opens new possibilities for tackling the problem using powerful data-driven deep learning methods.

In this paper, a deep residual neural network architecture is designed to remove clouds from multispectral Sentinel-2 imagery. SAR-optical data fusion is used to exploit the synergistic properties of the two imaging systems to guide the image reconstruction. Additionally, a novel cloud-adaptive loss is proposed to maximize the retainment of original information. The network is trained and tested on a globally sampled dataset comprising real cloudy and cloud-free images. The proposed setup allows to remove even optically thick clouds by reconstructing an optical representation of the underlying land surface structure.},
	language = {en},
	urldate = {2024-05-16},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Meraner, Andrea and Ebel, Patrick and Zhu, Xiao Xiang and Schmitt, Michael},
	month = aug,
	year = {2020},
	pages = {333--346},
}

@article{zheng_single_2021,
	title = {Single {Image} {Cloud} {Removal} {Using} {U}-{Net} and {Generative} {Adversarial} {Networks}},
	volume = {59},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0196-2892, 1558-0644},
	url = {https://ieeexplore.ieee.org/document/9224941/},
	doi = {10.1109/TGRS.2020.3027819},
	abstract = {Cloud removal is a ubiquitous and important task in remote sensing image processing, which aims at restoring the ground regions shadowed by clouds. It is challenging to remove the clouds for a single satellite image due to the difficulty of distinguishing clouds from white objects on the ground and filling the irregular missing regions with visual consistency. In this article, we propose a novel two-stage cloud removal method. The first stage is cloud segmentation, i.e., extracting the clouds and removing the thin clouds directly using U-Net. The second stage is image restoration, i.e., removing the thick cloud and recovering the corresponding irregular missing regions using generative adversarial network (GAN). We evaluate the proposed scheme on both synthetic images and real satellite images (over 20000×20000 pixels). On synthetic images for cloud coverage less than 40\%, the proposed scheme achieves improvements of 0.049–0.078 in Structural SIMilarity (SSIM) and 3.8–6.2 dB in peak signal-to-noise ratio (PSNR), while the ℓ1 -norm error reduces by 49\%–78\%, compared with a state-of-the-art deep learning method Pix2Pix. On real satellite images, we demonstrate the consistent visual results of the proposed scheme.},
	number = {8},
	urldate = {2024-05-16},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Zheng, Jiahao and Liu, Xiao-Yang and Wang, Xiaodong},
	month = aug,
	year = {2021},
	pages = {6371--6385},
}

@article{isola_image--image_2016,
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1611.07004},
	doi = {10.48550/ARXIV.1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	urldate = {2024-05-16},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	year = {2016},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
}

@inproceedings{grohnfeldt_conditional_2018,
	address = {Valencia},
	title = {A {Conditional} {Generative} {Adversarial} {Network} to {Fuse} {Sar} {And} {Multispectral} {Optical} {Data} {For} {Cloud} {Removal} {From} {Sentinel}-2 {Images}},
	isbn = {9781538671504},
	url = {https://ieeexplore.ieee.org/document/8519215/},
	doi = {10.1109/IGARSS.2018.8519215},
	abstract = {In this paper, we present the first conditional generative adversarial network (cGAN) architecture that is specifically designed to fuse synthetic aperture radar (SAR) and optical multi-spectral (MS) image data to generate cloud- and haze-free MS optical data from a cloud-corrupted MS input and an auxiliary SAR image. Experiments on Sentinel-2 MS and Sentinel-l SAR data confirm that our extended SAR-Opt-cGAN model utilizes the auxiliary SAR information to better reconstruct MS images than an equivalent model which uses the same architecture but only single-sensor MS data as input.},
	urldate = {2024-05-16},
	booktitle = {{IGARSS} 2018 - 2018 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	publisher = {IEEE},
	author = {Grohnfeldt, Claas and Schmitt, Michael and Zhu, Xiaoxiang},
	month = jul,
	year = {2018},
	pages = {1726--1729},
}

@article{gao_cloud_2020,
	title = {Cloud {Removal} with {Fusion} of {High} {Resolution} {Optical} and {SAR} {Images} {Using} {Generative} {Adversarial} {Networks}},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/12/1/191},
	doi = {10.3390/rs12010191},
	abstract = {The existence of clouds is one of the main factors that contributes to missing information in optical remote sensing images, restricting their further applications for Earth observation, so how to reconstruct the missing information caused by clouds is of great concern. Inspired by the image-to-image translation work based on convolutional neural network model and the heterogeneous information fusion thought, we propose a novel cloud removal method in this paper. The approach can be roughly divided into two steps: in the first step, a specially designed convolutional neural network (CNN) translates the synthetic aperture radar (SAR) images into simulated optical images in an object-to-object manner; in the second step, the simulated optical image, together with the SAR image and the optical image corrupted by clouds, is fused to reconstruct the corrupted area by a generative adversarial network (GAN) with a particular loss function. Between the first step and the second step, the contrast and luminance of the simulated optical image are randomly altered to make the model more robust. Two simulation experiments and one real-data experiment are conducted to confirm the effectiveness of the proposed method on Sentinel 1/2, GF 2/3 and airborne SAR/optical data. The results demonstrate that the proposed method outperforms state-of-the-art algorithms that also employ SAR images as auxiliary data.},
	language = {en},
	number = {1},
	urldate = {2024-05-16},
	journal = {Remote Sensing},
	author = {Gao, Jianhao and Yuan, Qiangqiang and Li, Jie and Zhang, Hai and Su, Xin},
	month = jan,
	year = {2020},
	pages = {191},
}

@article{xiong_sar--optical_2023,
	title = {{SAR}-to-{Optical} {Image} {Translation} and {Cloud} {Removal} {Based} on {Conditional} {Generative} {Adversarial} {Networks}: {Literature} {Survey}, {Taxonomy}, {Evaluation} {Indicators}, {Limits} and {Future} {Directions}},
	volume = {15},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2072-4292},
	shorttitle = {{SAR}-to-{Optical} {Image} {Translation} and {Cloud} {Removal} {Based} on {Conditional} {Generative} {Adversarial} {Networks}},
	url = {https://www.mdpi.com/2072-4292/15/4/1137},
	doi = {10.3390/rs15041137},
	abstract = {Due to the limitation of optical images that their waves cannot penetrate clouds, such images always suffer from cloud contamination, which causes missing information and limitations for subsequent agricultural applications, among others. Synthetic aperture radar (SAR) is able to provide surface information for all times and all weather. Therefore, translating SAR or fusing SAR and optical images to obtain cloud-free optical-like images are ideal ways to solve the cloud contamination issue. In this paper, we investigate the existing literature and provides two kinds of taxonomies, one based on the type of input and the other on the method used. Meanwhile, in this paper, we analyze the advantages and disadvantages while using different data as input. In the last section, we discuss the limitations of these current methods and propose several possible directions for future studies in this field.},
	language = {en},
	number = {4},
	urldate = {2024-05-16},
	journal = {Remote Sensing},
	author = {Xiong, Quan and Li, Guoqing and Yao, Xiaochuang and Zhang, Xiaodong},
	month = feb,
	year = {2023},
	pages = {1137},
}

@article{darbaghshahi_cloud_2022,
	title = {Cloud {Removal} in {Remote} {Sensing} {Images} {Using} {Generative} {Adversarial} {Networks} and {SAR}-to-{Optical} {Image} {Translation}},
	volume = {60},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0196-2892, 1558-0644},
	url = {https://ieeexplore.ieee.org/document/9627647/},
	doi = {10.1109/TGRS.2021.3131035},
	abstract = {Satellite images are often contaminated by clouds. Cloud removal has received special attention due to the wide range of satellite image applications. As the clouds thicken, the process of removing them becomes more challenging. In such cases, using auxiliary images, such as near-infrared or synthetic aperture radar (SAR), for reconstructing is common. In this study, we attempt to solve the problem using two generative adversarial networks (GANs): the first translates SAR images to optical images and the second removes clouds using the translated images of prior GAN. Also, we propose dilated residual inception blocks (DRIBs) instead of vanilla U-net in the generator networks and use structural similarity index measure (SSIM) in addition to the L1 loss function. Reducing the number of downsamplings and expanding receptive fields by dilated convolutions increased the quality of output images. We used the SEN1-2 dataset to train and test both GANs, and we made cloudy images by adding synthetic clouds to optical images. In addition, we used the SEN12MS-CR dataset to test network performance to remove real clouds. The restored images are evaluated using PSNR, SSIM, SAM, MAE, RMSE, and Q . We compared the proposed method with state-of-the-art deep learning models and achieved more accurate results in both SAR-to-optical image translation and cloud removal parts.},
	urldate = {2024-05-15},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Darbaghshahi, Faramarz Naderi and Mohammadi, Mohammad Reza and Soryani, Mohsen},
	year = {2022},
	pages = {1--9},
}

@article{zhang_cloud_2023,
	title = {Cloud removal using {SAR} and optical images via attention mechanism-based {GAN}},
	volume = {175},
	issn = {01678655},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167865523002623},
	doi = {10.1016/j.patrec.2023.09.014},
	abstract = {Clouds often appear in remote sensing images, which seriously affect the application of remote sensing images.
Therefore, cloud removal is an important preprocessing process in remote sensing image applications. In this paper, we propose a generative adversarial network-based cloud removal method for optical remote sensing images with the assistance of synthetic aperture radar (SAR) images. Our model is an end-to-end model, which consists of a translation module, an attention module, a generator, and a discriminator. We introduce the attention mechanism to accurately locate the cloud regions. With the obtained attention maps as the prior information, the proposed method can remove the clouds while preserving the cloud-free regions. In addition, we include the structural similarity index (SSIM) and the attention penalty in the loss function to improve the performance of the proposed method. Numerical experiments show that the proposed model provides improved cloud removal performance compared with the state-of-the-art methods.},
	language = {en},
	urldate = {2024-05-15},
	journal = {Pattern Recognition Letters},
	author = {Zhang, Shuai and Li, Xiaodi and Zhou, Xingyu and Wang, Yuning and Hu, Yue},
	month = nov,
	year = {2023},
	pages = {8--15},
}

@article{Beck2016Visual,
  abstract = {Bibiographic data such as collections of scientific articles and citation networks have been studied extensively in information visualization and visual analytics research. Powerful systems have been built to support various types of bibliographic analysis, but they require some training and cannot be used to disseminate the insights gained. In contrast, we focused on developing a more accessible visual analytics system, called SurVis, that is ready to disseminate a carefully surveyed literature collection. The authors of a survey may use our Web-based system to structure and analyze their literature database. Later, readers of the survey can obtain an overview, quickly retrieve specific publications, and reproduce or extend the original bibliographic analysis. Our system employs a set of selectors that enable users to filter and browse the literature collection as well as to control interactive visualizations. The versatile selector concept includes selectors for textual search, filtering by keywords and meta-information, selection and clustering of similar publications, and following citation links. Agreement to the selector is represented by word-sized sparkline visualizations seamlessly integrated into the user interface. Based on an analysis of the analytical reasoning process, we derived requirements for the system. We developed the system in a formative way involving other researchers writing literature surveys. A questionnaire study with 14 visual analytics experts confirms that SurVis meets the initially formulated requirements.},
  author = {Beck, Fabian and Koch, Sebastian and Weiskopf, Daniel},
  doi = {10.1109/TVCG.2015.2467757},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  keywords = {type:system, visual_analytics, sparklines, information_retrieval, clustering, literature_browser},
  number = {01},
  publisher = {IEEE},
  volume = {22},
  series = {TVCG},
  title = {Visual Analysis and Dissemination of Scientific Literature Collections with {SurVis}},
  url = {http://www.visus.uni-stuttgart.de/uploads/tx_vispublications/vast15-survis.pdf},
  year = {2016}
}
